#    https://colab.research.google.com/drive/1yx0kF2tOYYi8SrPCVVW7V0Rax2jGAhh9

# å®‰è£…ä¾èµ–åŒ…
!pip install elevenlabs pydub

import re
import os
import io
from pathlib import Path
from typing import Dict, List, Tuple
from elevenlabs import ElevenLabs, VoiceSettings
from pydub import AudioSegment
import json
from datetime import datetime

# ğŸ”‘ API Key
# https://elevenlabs.io/app/settings/api-keys
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY", "your_api_key_here")

# ğŸ™ï¸ éŸ³é¢‘è®¾å®š
# æ¯æ®µè¯ä¹‹é—´çš„é—´éš”èŒƒå›´ (milliseconds)
# ç”Ÿæˆæ—¶ï¼Œä¼šéšæœºåœ¨ä»¥ä¸‹èŒƒå›´é€‰å–ä¸€ä¸ªéšæœºå€¼
PAUSE_RANGE_MS = (600, 1000)

# æ˜¯å¦ä¸‹è½½åˆ†æ®µéŸ³é¢‘æ–‡ä»¶
DOWNLOAD_CHUNKS = False

# æ¨¡å‹
# v3 ä»¥ä¸‹çš„æ¨¡å‹ä¼¼ä¹ä¸­è‹±å¤¹æ‚éƒ½ä¸å¤ªè¡Œ
# https://elevenlabs.io/docs/overview/models#character-limits
MODEL_ID="eleven_v3"

# å£°éŸ³ç”Ÿæˆå‚æ•°ï¼Œå‚è€ƒæ–‡æ¡£ï¼š
# https://elevenlabs.io/docs/api-reference/text-to-speech/convert#request.body.voice_settings
VOICE_SETTINGS = VoiceSettings(
    stability=0.5,
    similarity_boost=0.75,
    speed=1.0,
    style=0.0,
    use_speaker_boost=True
)

# ğŸ™ï¸ é»˜è®¤å£°éŸ³åº“
# å¦‚æœä¸æŒ‡å®šæ¯ä¸ª speaker çš„å£°éŸ³ï¼Œå°±ä¼šä»ä»¥ä¸‹åˆ—è¡¨é‡Œä¾æ¬¡é€‰å–
# æµè§ˆæ‰€æœ‰å£°éŸ³ï¼šhttps://elevenlabs.io/app/voice-library
DEFAULT_VOICES = [
    # Lucy
    # https://elevenlabs.io/app/voice-library?voiceId=lcMyyd2HUfFzxdCaC4Ta
    "lcMyyd2HUfFzxdCaC4Ta",
    # Zhile
    # https://elevenlabs.io/app/voice-library?voiceId=bdt3B5N3GXM2nOc0SUW7
    "bdt3B5N3GXM2nOc0SUW7",
    # Evan Zhao
    # https://elevenlabs.io/app/voice-library?voiceId=MI36FIkp9wRP7cpWKPTl
    "MI36FIkp9wRP7cpWKPTl",
    # Chihiro Yoko
    # https://elevenlabs.io/app/voice-library?voiceId=NIqnuIdrAT3LLSSxN05L
    "NIqnuIdrAT3LLSSxN05L",
    # James Gao
    # https://elevenlabs.io/app/voice-library?voiceId=4VZIsMPtgggwNg7OXbPY
    "4VZIsMPtgggwNg7OXbPY",
    # Stacy
    # https://elevenlabs.io/app/voice-library?voiceId=hkfHEbBvdQFNX4uWHqRF
    "hkfHEbBvdQFNX4uWHqRF",
    # Julia
    # https://elevenlabs.io/app/voice-library?voiceId=tOuLUAIdXShmWH7PEUrU
    "tOuLUAIdXShmWH7PEUrU",
    # Angela
    # https://elevenlabs.io/app/voice-library?voiceId=FUfBrNit0NNZAwb58KWH
    "FUfBrNit0NNZAwb58KWH",
]

if ELEVENLABS_API_KEY == "your_api_key_here":
    print("âŒ Please set your ElevenLabs API key in ELEVENLABS_API_KEY")
else:
    print("âœ… API key configured")
print(f"âœ… Default voice pool has {len(DEFAULT_VOICES)} voices")

# ç»™ script é‡Œçš„æ¯ä¸ª speaker æŒ‡å®šå£°éŸ³
VOICE_MAP = {
    "HarbKidsFun": "RILOU7YmBhvwJGDGjNmP",
    "Aè€å¸ˆ": "lcMyyd2HUfFzxdCaC4Ta",
    "Bè€å¸ˆ": "tOuLUAIdXShmWH7PEUrU",
}

# æ¯ä¸ª speaker ä¹‹é—´éœ€è¦æœ‰é¢å¤–ç©ºè¡Œ
# æ¯ä¸ª speaker å¼€å¤´ï¼Œå¿…é¡»ç´§è·Ÿè‹±æ–‡æˆ–è€…ä¸­æ–‡å†’å·
SCRIPT = """
HarbKidsFunï¼š å“ˆå–½å¤§å®¶æ–°å¹´å¥½å‘€ï¼æ¬¢è¿æ¥åˆ° HarbKidsFun 2026å¹´çš„ç¬¬ä¸€æœŸè®¿è°ˆã€‚ä»Šå¤©å‘¢ï¼Œæˆ‘ä»¬ç‰¹åˆ«å¹¸è¿åœ°è¿çº¿åˆ°äº†ä¸¤ä½æ•™è¿‡æ±‰åŸºâ€”â€”å°±æ˜¯å¤§å®¶å¸¸è¯´çš„CISâ€”â€”å°å­¦éƒ¨çš„ä¸­æ–‡è€å¸ˆã€‚ä¸€ä½æ˜¯ç°åœ¨è¿˜åœ¨èŒçš„Aè€å¸ˆï¼Œå¦ä¸€ä½æ˜¯å·²ç»ç¦»èŒçš„Bè€å¸ˆã€‚æˆ‘ä»¬æƒ³è¯·å¥¹ä»¬æ¥è·Ÿå¤§å®¶éšä¾¿èŠèŠæ±‰åŸºå°å­¦é˜¶æ®µï¼Œå°±æ˜¯ä»Receptionå­¦å‰ç­åˆ°Yå…­è¿™æ®µçš„ä¸­æ–‡æ•™å­¦ç‰¹è‰²ã€‚è¿™ä¹Ÿç®—æ˜¯ç»§å»å¹´æ¨å‡ºäº†å¥½å‡ ç¯‡æ¸¯æ¼‚å¦ˆå¦ˆæ‹©æ ¡è®¿è°ˆä¹‹åï¼Œæˆ‘ä»¬å¼€å§‹æŠŠè®¿è°ˆå¯¹è±¡æ‰©å±•åˆ°é¦™æ¸¯æœ¬åœ°çš„è€å¸ˆç¾¤ä½“äº†ã€‚æ–°çš„ä¸€å¹´é‡Œå‘¢ï¼Œåœ¨æ‹©æ ¡è¿™æ¡èµ„è®¯çº¿ä¸Šï¼Œæˆ‘ä»¬åœ¨æ¨å‡ºæ›´å¤šçœŸäººè®¿è°ˆçš„åŒæ—¶ï¼Œä¹Ÿä¼šåŠªåŠ›æ¶µç›–é¦™æ¸¯å­¦æ ¡ç”Ÿæ€åœˆé‡Œæ›´å¤šä¸åŒçš„è§’è‰²ï¼Œå¤§å®¶æ•¬è¯·æœŸå¾…å“ˆã€‚

HarbKidsFunï¼š ä¸¤ä½è€å¸ˆæ–°å¹´å¥½ï¼å¤§å®¶éƒ½çŸ¥é“æ±‰åŸºæ˜¯é¦™æ¸¯å›½é™…å­¦æ ¡é‡Œçš„é¡¶æµå˜›ï¼Œç‰¹åˆ«æ˜¯ä»¥æ‰å®çš„åŒè¯­æ•™å­¦å‡ºåï¼Œè€Œä¸¤ä½è€å¸ˆéƒ½æ˜¯è¿™é‡Œé¢çš„ä¸»åŠ›å†›ï¼Œæ‰€ä»¥æˆ‘ä»¬çœŸçš„ç‰¹åˆ«è£å¹¸èƒ½æœ‰æœºä¼šç›´æ¥å‘ä¸¤ä½è€å¸ˆè¯·æ•™ã€‚èƒ½ä¸èƒ½å…ˆè¯·ä½ ä»¬ä»‹ç»ä¸€ä¸‹å„è‡ªåœ¨æ±‰åŸºçš„å·¥ä½œç»å†å‘¢ï¼Ÿ

Aè€å¸ˆï¼š å¤§å®¶å¥½ã€‚æˆ‘ç›®å‰åœ¨æ±‰åŸºå°å­¦éƒ¨ä¸»è¦è´Ÿè´£ä¸‰å—å·¥ä½œï¼šç¬¬ä¸€ï¼Œæ‹…ä»»æŸä¸¤ä¸ªå¹´çº§çš„ä¸­æ–‡è¯¾ç¨‹ä¸»ä»»ï¼›ç¬¬äºŒï¼Œè´Ÿè´£å…¶ä¸­ä¸€ä¸ªå¹´çº§çš„ä¸­æ–‡è¯¾å ‚æ•™å­¦ï¼›ç¬¬ä¸‰ï¼Œæ‹…ä»»è¯¥å¹´çº§æŸç­çš„ä¸­æ–‡ Homeroom Teacherï¼Œä¹Ÿå°±æ˜¯ä¸­æ–‡ç­ä¸»ä»»ã€‚

Bè€å¸ˆï¼š å¤§å®¶å¥½å‘€ï¼æˆ‘å‘¢ï¼Œæ˜¯åœ¨å‡ å¹´å‰ç¬¬äºŒä¸ªå­©å­å‡ºç”Ÿåç¦»å¼€äº†å·¥ä½œåå¤šå¹´çš„æ±‰åŸºï¼Œç°åœ¨è¿˜æ˜¯åœ¨å®¶å¸¦å¨ƒçš„çŠ¶æ€ã€‚åœ¨æ±‰åŸºçš„é‚£åå¤šå¹´é‡Œï¼Œæˆ‘ä¸€å¼€å§‹æ˜¯æ‹…ä»» Receptionâ€”â€”å°±æ˜¯å­¦å‰ç­ï¼Œæ”¶ç”Ÿå¹´é¾„å¯¹åº”é¦™æ¸¯æœ¬åœ°å­¦æ ¡KäºŒâ€”â€”çš„ä¸­æ–‡ç­ä¸»ä»»ï¼Œåæ¥è½¬å»åš Yä¸€åˆ°Yå…­ çš„ä¸­æ–‡æ”¯æ´è€å¸ˆï¼Œä¸»è¦æ˜¯ç»™ä¸­æ–‡ç¨‹åº¦ç›¸å¯¹è½åçš„å­©å­æä¾› Foundationæˆ–è€…Ab initio çš„è¾…å¯¼è¯¾ç¨‹ã€‚

HarbKidsFunï¼š çœ‹æ¥ä¸¤ä½è€å¸ˆçš„å·¥ä½œèŒƒå›´å’Œä»»èŒæ—¶é—´éƒ½æœ‰äº›ä¸ä¸€æ ·å“ˆã€‚æˆ‘ä»¬å…ˆæ¥äº†è§£ä¸€ä¸‹ï¼Œç°åœ¨æ±‰åŸºå°å­¦éƒ¨å„å¹´çº§çš„äººæ•°æ˜¯æ€æ ·çš„ï¼Ÿ
"""

def parse_script(script_text: str) -> List[Tuple[str, str]]:
    """Parse script into (speaker, text) tuples."""
    dialog_turns = []

    # Split by double newlines (paragraphs)
    paragraphs = [p.strip() for p in script_text.strip().split('\n\n') if p.strip()]

    # Pattern matches "Speaker: " or "Speakerï¼š " (Chinese colon)
    pattern = r'^([^:ï¼š]+)[ï¼š:]\s*(.+)$'

    for para in paragraphs:
        match = re.match(pattern, para, re.DOTALL)
        if match:
            speaker = match.group(1).strip()
            text = match.group(2).strip()
            dialog_turns.append((speaker, text))

    return dialog_turns

def get_unique_speakers(dialog_turns: List[Tuple[str, str]]) -> List[str]:
    """Extract unique speakers in order of first appearance."""
    speakers = []
    seen = set()
    for speaker, _ in dialog_turns:
        if speaker not in seen:
            speakers.append(speaker)
            seen.add(speaker)
    return speakers

# Parse and display
dialog_turns = parse_script(SCRIPT)
unique_speakers = get_unique_speakers(dialog_turns)

print(f"âœ… Parsed {len(dialog_turns)} dialog turns")
print(f"ğŸ“‹ Found {len(unique_speakers)} unique speakers:\n")
for i, speaker in enumerate(unique_speakers, 1):
    print(f"  {i}. {speaker}")

def assign_voices(
    speakers: List[str],
    voice_map: Dict[str, str],
    default_voices: List[str]
) -> Dict[str, str]:
    """
    Assign voices to all speakers.
    Speakers with explicit mappings keep their voices.
    Unmapped speakers get unique voices from default_voices pool.
    """
    unmapped_speakers = [s for s in speakers if s not in voice_map]

    # å¦‚æœé»˜è®¤å£°éŸ³åº“é‡Œçš„å£°éŸ³ä¸å¤Ÿï¼Œä¼šæŠ¥é”™
    if len(unmapped_speakers) > len(default_voices):
        raise ValueError(
            f"âŒ Not enough default voices!\n"
            f"   Unmapped speakers: {len(unmapped_speakers)}\n"
            f"   Available default voices: {len(default_voices)}\n"
            f"   Please add more voices to DEFAULT_VOICES or specify voices in VOICE_MAP"
        )

    # Create complete voice mapping
    complete_mapping = dict(voice_map)

    # Assign default voices to unmapped speakers
    for idx, speaker in enumerate(unmapped_speakers):
        complete_mapping[speaker] = default_voices[idx]

    # Check for duplicate voice assignments
    voice_to_speakers = {}
    for speaker, voice_id in complete_mapping.items():
        if voice_id not in voice_to_speakers:
            voice_to_speakers[voice_id] = []
        voice_to_speakers[voice_id].append(speaker)

    # å¦‚æœç»™ä¸åŒ speaker æŒ‡å®šçš„å£°éŸ³æœ‰é‡å¤ï¼Œä¼šæŠ¥é”™
    duplicates = {voice_id: speakers for voice_id, speakers in voice_to_speakers.items() if len(speakers) > 1}

    if duplicates:
        error_msg = "âŒ Multiple speakers are assigned the same voice:\n"
        for voice_id, speakers_list in duplicates.items():
            error_msg += f"   Voice {voice_id}:\n"
            for speaker in speakers_list:
                error_msg += f"     - {speaker}\n"
        error_msg += "\n   Each speaker must have a unique voice. Please update VOICE_MAP."
        raise ValueError(error_msg)

    return complete_mapping, unmapped_speakers

# Assign voices
try:
    complete_voice_map, unmapped_speakers = assign_voices(
        unique_speakers,
        VOICE_MAP,
        DEFAULT_VOICES
    )

    print("ğŸ™ï¸  Voice Assignments:\n")
    for speaker in unique_speakers:
        voice_id = complete_voice_map[speaker]
        if speaker in VOICE_MAP:
            status = "âœ“ (custom)"
        else:
            status = "âš™ï¸ (default)"
        print(f"  {status} {speaker}: {voice_id}")

    if unmapped_speakers:
        print(f"\nâ„¹ï¸  {len(unmapped_speakers)} speaker(s) using default voices from pool")

except ValueError as e:
    print(str(e))
    raise

def chunk_dialog_turns(
    dialog_turns: List[Tuple[str, str]],
    max_chunk_chars: int = 2000
) -> List[List[Tuple[str, str]]]:
    """
    Chunk dialog turns to avoid quality degradation.
    Merges consecutive turns from same speaker when possible.
    """
    chunks = []
    current_chunk = []
    current_length = 0

    for speaker, text in dialog_turns:
        text_length = len(text)

        # If adding this turn exceeds limit and we have content, start new chunk
        if current_length + text_length > max_chunk_chars and current_chunk:
            chunks.append(current_chunk)
            current_chunk = []
            current_length = 0

        # Merge with previous turn if same speaker
        if current_chunk and current_chunk[-1][0] == speaker:
            prev_speaker, prev_text = current_chunk[-1]
            merged_text = prev_text + "\n\n" + text
            current_chunk[-1] = (speaker, merged_text)
            current_length += len("\n\n") + text_length
        else:
            current_chunk.append((speaker, text))
            current_length += text_length

    if current_chunk:
        chunks.append(current_chunk)

    return chunks

def generate_dialog_audio(
    dialog_turns: List[Tuple[str, str]],
    voice_map: Dict[str, str],
    api_key: str,
    output_dir: str = "output",
    max_chunk_chars: int = 2000,
    pause_range_ms: Tuple[int, int] = PAUSE_RANGE_MS
):
    """Generate audio for dialog with chunking strategy."""

    import random

    # Initialize client
    client = ElevenLabs(api_key=api_key)

    # Create output directories
    output_path = Path(output_dir)
    chunks_dir = output_path / "chunks"
    chunks_dir.mkdir(parents=True, exist_ok=True)

    # Chunk the dialog
    chunked_turns = chunk_dialog_turns(dialog_turns, max_chunk_chars)

    print(f"ğŸ“Š Split into {len(chunked_turns)} chunks for generation\n")

    chunk_files = []
    metadata = {
        "generated_at": datetime.now().isoformat(),
        "total_chunks": len(chunked_turns),
        "voice_map": voice_map,
        "pause_range_ms": pause_range_ms,
        "chunks": []
    }

    # Generate each chunk
    for chunk_idx, chunk_turns in enumerate(chunked_turns, 1):
        print(f"ğŸ™ï¸  Generating chunk {chunk_idx}/{len(chunked_turns)}...")

        chunk_segments = []

        for turn_idx, (speaker, text) in enumerate(chunk_turns):
            voice_id = voice_map[speaker]

            print(f"   Turn {turn_idx + 1}: {speaker} ({len(text)} chars)")

            # Generate audio for this turn
            audio_generator = client.text_to_speech.convert(
                voice_id=voice_id,
                text=text,
                model_id=MODEL_ID,
                voice_settings=VOICE_SETTINGS
            )

            # Collect audio bytes
            audio_bytes = b"".join(audio_generator)

            # Save individual turn (optional, for debugging)
            turn_file = chunks_dir / f"chunk_{chunk_idx:03d}_turn_{turn_idx + 1:02d}_{speaker}.mp3"
            with open(turn_file, "wb") as f:
                f.write(audio_bytes)

            chunk_segments.append((speaker, audio_bytes))

        # Concatenate all turns in this chunk with random pauses
        chunk_audio = AudioSegment.empty()
        prev_speaker = None

        for idx, (speaker, audio_bytes) in enumerate(chunk_segments):
            segment = AudioSegment.from_mp3(io.BytesIO(audio_bytes))

            # Add random pause before this segment if speaker changed
            if prev_speaker is not None and prev_speaker != speaker:
                pause_duration = random.randint(pause_range_ms[0], pause_range_ms[1])
                silence = AudioSegment.silent(duration=pause_duration)
                chunk_audio += silence

            chunk_audio += segment
            prev_speaker = speaker

        # Save chunk
        chunk_file = chunks_dir / f"chunk_{chunk_idx:03d}.mp3"
        chunk_audio.export(chunk_file, format="mp3")
        chunk_files.append(chunk_file)

        metadata["chunks"].append({
            "chunk_id": chunk_idx,
            "file": str(chunk_file),
            "turns": len(chunk_turns),
            "duration_ms": len(chunk_audio)
        })

        print(f"   âœ… Saved: {chunk_file}\n")

    # Concatenate all chunks into final audio
    print("ğŸ¬ Creating final concatenated audio...")
    final_audio = AudioSegment.empty()
    for chunk_file in chunk_files:
        segment = AudioSegment.from_mp3(chunk_file)
        final_audio += segment

    # Save final output
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    final_file = output_path / f"dialog_{timestamp}.mp3"
    final_audio.export(final_file, format="mp3")

    # Save metadata
    metadata_file = output_path / f"dialog_{timestamp}_metadata.json"
    with open(metadata_file, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Complete!")
    print(f"ğŸ“ Final audio: {final_file}")
    print(f"ğŸ“Š Duration: {len(final_audio) / 1000:.1f} seconds")
    print(f"ğŸ“‹ Metadata: {metadata_file}")

    return final_file, metadata

# ç”Ÿæˆå¯¹è¯ï¼ˆè¿™ä¸€æ­¥å¯èƒ½è¦ç­‰å¾ˆé•¿æ—¶é—´ï¼‰
final_file, metadata = generate_dialog_audio(
    dialog_turns=dialog_turns,
    voice_map=complete_voice_map,
    api_key=ELEVENLABS_API_KEY,
    output_dir="output",
    pause_range_ms=PAUSE_RANGE_MS
)

from google.colab import files

# ä¸‹è½½æ–‡ä»¶
print("ğŸ“¥ ä¸‹è½½æœ€ç»ˆå¯¹è¯æ–‡ä»¶...")
files.download(str(final_file))

if DOWNLOAD_CHUNKS:
    print("ğŸ“¥ ä¸‹è½½åˆ†æ®µæ–‡ä»¶...")
    for chunk_file in Path("output/chunks").glob("chunk_*.mp3"):
        files.download(str(chunk_file))

print("\nâœ… All done! Check your downloads folder.")
